# -*- coding: utf-8 -*-
"""linear_algebra_PCA_analysis

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1v_x8fDqiM0TvBuM6EMNWLyFOp7rDuETo

**Import** **libraries**
"""

from re import X
import numpy as np
import csv
from numpy.core.defchararray import encode
from numpy.core.numeric import NaN
from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix,  f1_score
from sklearn.preprocessing import Normalizer
from sklearn.model_selection import GridSearchCV
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
import warnings
warnings.filterwarnings("ignore")

"""**Read Data**"""

import pandas as pd 
def data_loader(train_path, test_path):
    data = pd.read_csv(train_path)
    data_top = list(data.columns)
    with open(train_path, 'r') as fp:     
        data_train = list(csv.reader(fp))
        train_id = np.array(data_train[1:])[:, :1]
        data_train = np.array(data_train[1:])[:, 1:]
        
    with open(test_path, 'r') as fp:     
        data_test = list(csv.reader(fp))
        test_id = np.array(data_test[1:])
        data_test = np.array(data_test[1:])[:, 1:]
        
    
    return data_train, train_id, data_test, test_id,data_top

"""**Encode Y**"""

def encode_y(data):
    from sklearn.preprocessing import OrdinalEncoder
    enc = OrdinalEncoder(categories=[['No Churn', 'Competitor', 'Dissatisfaction', 'Attitude', 'Price', 'Other']])
    y = enc.fit_transform(data[:, 44:45])
    for i in range((len(y))):
        y[i] = int(y[i])
    return y

"""**Encode Other Features**"""

def encode_other(data, feats):
    from sklearn.preprocessing import OrdinalEncoder
    enc = OrdinalEncoder()
    for i in feats:
        data[:, i:i+1] = enc.fit_transform(data[:, i:i+1]) # 35 contract
    return data

"""**Extract Numerical Features**"""

def feature_extractor(data_trainval, data_test, feats,bb):
    x = data_trainval[:, feats]
    x = (x.astype(np.float))
    #x = np.nan_to_num(x)

    x_test = data_test[:, feats]
    x_test = (x_test.astype(np.float))
    if bb==1:
      x = np.nan_to_num(x)
      x_test = np.nan_to_num(x_test)
           
    elif bb==2: #nan as avg 
      
      xmean = np.mean(np.nan_to_num(x), dtype=np.float64)
      x_testmean = np.mean(np.nan_to_num(x_test), dtype=np.float64)
      where_are_NaNs = np.where(np.isnan(x))
      x[where_are_NaNs] = xmean
      where_are_NaNst = np.where(np.isnan(x_test))
      x_test[where_are_NaNst] = x_testmean
    elif bb==3: #nan as a new type      
      where_are_NaNs = np.where(np.isnan(x))
      x[where_are_NaNs] = 666
      where_are_NaNst = np.where(np.isnan(x_test))      
      x_test[where_are_NaNst] = 666
    

    return x, x_test

"""**Normalize**"""

def normalize(data_trainval, data_test):
    scaler = Normalizer().fit(data_trainval)
    data_trainval = scaler.transform(data_trainval)
    scaler = Normalizer().fit(data_test)
    data_test = scaler.transform(data_test)
    return data_trainval, data_test

"""**Beta Encoder**"""

class BetaEncoder(object):

  def __init__(self, group):
    self.group = group
    self.stats = None

  def fit(self, X_train, target_col):
    self.prior_mean = np.mean(X_train[target_col])
    stats = X_train[[target_col, self.group]].groupby(self.group)
    stats = stats.agg(['sum', 'count'])[target_col]    
    stats.rename(columns={'sum': 'n', 'count': 'N'}, inplace=True)
    stats.reset_index(level=0, inplace=True)           
    self.stats = stats

  def transform(self, X_train, stat_type, N_min=1):
        
    X_train_stats = np.hstack(X_train[[self.group]], self.stats, how='left')
    n = X_train_stats['n'].copy()
    N = X_train_stats['N'].copy()
    
    # fill in missing
    nan_indexs = np.isnan(n)
    n[nan_indexs] = self.prior_mean
    N[nan_indexs] = 1.0
    
    # prior parameters
    N_prior = np.maximum(N_min-N, 0)
    alpha_prior = self.prior_mean*N_prior
    beta_prior = (1-self.prior_mean)*N_prior
    
    # posterior parameters
    alpha = alpha_prior + n
    beta =  beta_prior + N-n
    
    # calculate statistics
    if stat_type=='mean':
        num = alpha
        dem = alpha+beta
                
    elif stat_type=='mode':
        num = alpha-1
        dem = alpha+beta-2
        
    elif stat_type=='median':
        num = alpha-1/3
        dem = alpha+beta-2/3
    
    elif stat_type=='var':
        num = alpha*beta
        dem = (alpha+beta)**2*(alpha+beta+1)
                
    elif stat_type=='skewness':
        num = 2*(beta-alpha)*np.sqrt(alpha+beta+1)
        dem = (alpha+beta+2)*np.sqrt(alpha*beta)

    elif stat_type=='kurtosis':
        num = 6*(alpha-beta)**2*(alpha+beta+1) - alpha*beta*(alpha+beta+2)
        dem = alpha*beta*(alpha+beta+2)*(alpha+beta+3)

    else:
        num = self.prior_mean
        dem = np.ones_like(N_prior)
        
    # replace missing
    value = num/dem
    value[np.isnan(value)] = np.nanmedian(value)
    return value

"""**Upsampling**"""

def upsampling(X_train, y_train):
  # upsampling on training data only
  from imblearn.over_sampling import SMOTE
  smt = SMOTE(random_state = 1126)
  X_train, y_train = smt.fit_resample(X_train, y_train)
  from collections import Counter
  print(sorted(Counter(y_train).items()))
  return X_train, y_train

"""**Undersampling**"""

def undersampling(X_train, y_train):
  # downsampling on data only
  from imblearn.under_sampling import RandomUnderSampler
  rus = RandomUnderSampler(random_state=1126)
  X_train, y_train = rus.fit_resample(X_train, y_train)
  from collections import Counter
  print(sorted(Counter(y_train).items()))
  return X_train, y_train

"""**Naive Bayes classifier**"""

def NB(X_train, X_val, y_train, y_val):
    from sklearn.naive_bayes import GaussianNB
    gnb = GaussianNB().fit(X_train, y_train)
    val_predictions = gnb.predict(X_val)

    # Validation accuracy
    accuracy = gnb.score(X_val, y_val)
    print('NB accuracy')
    print(accuracy)
    # Validation confusion matrix
    cm = confusion_matrix(y_val, val_predictions)
    print('NB CFmap')
    print(cm)

    #testing
    test_predictions = gnb.predict(x_test)
    return test_predictions

"""**KNN**"""

def KNN(X_train, X_val, y_train, y_val):
    from sklearn.neighbors import KNeighborsClassifier
    knn = KNeighborsClassifier(n_neighbors = 7).fit(X_train, y_train)
    # model accuracy for X_test 
    accuracy = knn.score(X_val, y_val)
    print('KNN accuracy')
    print(accuracy)
    # creating a confusion matrix
    knn_predictions = knn.predict(X_val)
    cm = confusion_matrix(y_val, knn_predictions)
    print('KNN CFmap')
    print(cm)
    #testing
    test_predictions = knn.predict(x_test)
    return test_predictions

"""**SVM**"""

def SVM(X_train, X_val, y_train, y_val, grid = False):
    from sklearn.svm import SVC
    
    # grid search
    if grid == True:
        svm = SVC()
        parameters = {'kernel':['rbf'],
                    'gamma': [ 1e-4, 1e-3, 1e-2], #, 1e-1, 1
                    'C': [2e7, 2e8, 2e9, 2e10, 2e11, 2e12, 2e13], #
                    }
        clf = GridSearchCV(svm, parameters)#  scoring=['recall_macro', 'precision_macro'], refit=False
        clf.fit(X_train, y_train)
        print(clf.best_params_)
        svm = clf.best_estimator_
    
    if grid == False:
        svm = SVC(kernel = 'rbf', C = 1)
        svm.fit(X_train, y_train)

    svm_predictions = svm.predict(X_val)
    accuracy = svm.score(X_val, y_val)
    print('SVM accuracy')
    print(accuracy)
    cm = confusion_matrix(y_val, svm_predictions)
    print('SVM CFmap')
    print(cm)
    print('RF f1 score')
    print(f1_score(y_val, svm_predictions, average = 'macro'))
    #testing
    test_predictions = svm.predict(x_test)
    return test_predictions

"""**Random Forest**"""

def RF(X_train, X_val, y_train, y_val, grid = False):
    from sklearn.ensemble import RandomForestClassifier

    #grid search
    if grid == True:
        rf = RandomForestClassifier()
        parameters = {'n_estimators':range(100, 1000, 100)}
        clf = GridSearchCV(rf, parameters)
        clf.fit(X_train, y_train)
        rf = clf.best_estimator_

    if grid == False:
        rf = RandomForestClassifier()
        rf.fit(X_train, y_train)
    
    #result
    rf_predictions = rf.predict(X_val)
    accuracy = rf.score(X_val, y_val)
    print('RF accuracy')
    print(accuracy)
    cm = confusion_matrix(y_val, rf_predictions)
    print('RF CFmap')
    print(cm)
    print('RF f1 score')
    print(f1_score(y_val, rf_predictions, average = 'macro'))
    #testing
    test_predictions = rf.predict(x_test)
    return test_predictions

"""**Gradient Boost**"""

def GradientBoosting(X_train, X_val, y_train, y_val, grid = False):
    from sklearn.ensemble import GradientBoostingClassifier

    if grid == True:
        gb = GradientBoostingClassifier()
        parameters = {'n_estimators': range(100, 500, 100),
                      'learning_rate': [0.01, 0.05, 0.1, 0.5, 1]
                      }
        clf = GridSearchCV(gb, parameters)
        clf.fit(X_train, y_train)
        print(clf.best_params_)
        gb = clf.best_estimator_

    else:
        gb = GradientBoostingClassifier(n_estimators = 100, learning_rate = 0.1, random_state = 1126)
        gb = Pipeline([ #('pca', PCA(n_components = 5)),
                        ('clf', gb)
                    ])
        gb.fit(X_train, y_train)

    # Validation
    accuracy = gb.score(X_val, y_val)
    print('GB accuracy')
    print(accuracy)
    # creating a confusion matrix
    gb_predictions = gb.predict(X_val)
    cm = confusion_matrix(y_val, gb_predictions)
    print('GB CFmap')
    print(cm)
    print('GB f1 score')
    print(f1_score(y_val, gb_predictions, average = 'macro'))
    # add validation-set into sub-training set
    X_trainval = np.vstack((np.array(X_train), np.array(X_val)))
    y_trainval = np.hstack((np.array(y_train), np.array(y_val)))

    # retraining w/ full training set
    if grid == True:
        gb = GradientBoostingClassifier(clf.best_params_)
    else:
        gb = GradientBoostingClassifier(n_estimators = 400, learning_rate = 0.5, random_state = 1126)

    gb.fit(X_trainval, y_trainval)
    # testing
    test_predictions = gb.predict(x_test)
    return test_predictions

"""**Output Prediction**"""

def make_pred(pred, test_id):
    with open('pred.csv', 'w', newline='') as fp:
        writer = csv.writer(fp)
        writer.writerow(['Customer ID', 'Churn Category'])
        for i, p in enumerate(pred): 
            writer.writerow([test_id[i, 0], int(p)])

"""**Download Data**"""

train_path = 'data.train.csv'  # path to training data
test_path = 'data.test.csv'
!gdown --id '1_umR9yNNBZLYourueYsPGeLR7IQLOuy1' --output data.train.csv
!gdown --id '1Q-8zWSrDT8eDlQfDGup2LSIQ0RwMvLg_' --output data.test.csv

"""**Main**"""

from pandas import read_csv
from sklearn.feature_selection import RFE
from sklearn.linear_model import LogisticRegression
from sklearn.feature_selection import SelectKBest
from sklearn.feature_selection import f_classif
from numpy import set_printoptions
from sklearn.datasets import load_digits
from sklearn.feature_selection import SelectKBest, chi2
from sklearn.feature_selection import SelectFromModel
from sklearn.linear_model import LogisticRegression
from sklearn.feature_selection import RFE
import pandas as pd

data_trainval, train_id, data_test, test_id ,feature_names = data_loader(train_path,test_path)
feanames = []
for i in range(len(feature_names)-1):
  feanames.append(feature_names[i+1])


y = np.ravel(encode_y(data_trainval))

data_trainval = encode_other(data_trainval, [1,8,9,10,12,14,16,17,20,21,23,24,25]+list(range(3,7))+list(range(27,38)))
data_test = encode_other(data_test,[1,8,9,10,12,14,16,17,20,21,23,24,25]+list(range(3,7))+list(range(27,38)))

x_trainval, x_test = feature_extractor(data_trainval, data_test, list(range(0,44)),1)


df = pd.DataFrame(x_trainval)

from sklearn.ensemble import RandomForestClassifier
import pandas as pd
import numpy as np
import warnings,math
import matplotlib.pyplot as plt

from sklearn.preprocessing import StandardScaler
stdsc = StandardScaler()
X_train_std = stdsc.fit_transform(df)

#求共變異係數矩陣
cov_mat = np.cov(X_train_std.T)
print("共變異係數矩陣.shape=",cov_mat.shape)
print("共變異係數矩陣=",cov_mat)

#求共變異係數矩陣 的特徵向量及特徵值
eigen_vals, eigen_vecs = np.linalg.eig(cov_mat)
print("特徵向量.shape=",eigen_vecs.shape)
print("特徵向量=",eigen_vecs)
print("特徵值=",eigen_vals)

#計算解釋變異數比率 各特徵值/特徵值總和
tot = sum(eigen_vals)
var_exp = [(i / tot) for i in sorted(eigen_vals, reverse=True)]
cum_var_exp = np.cumsum(var_exp)
print("各特徵值變異數比率：",var_exp)
print("特徵值變異數比率累加：",cum_var_exp)

#畫圖 ：解釋變異數比率 ，各特徵值/特徵值總和
plt.bar(range(0, 44), var_exp, alpha=0.5, align='center',
        label='individual explained variance')
plt.step(range(0, 44), cum_var_exp, where='mid',
         label='cumulative explained variance')
plt.ylabel('Explained variance ratio')
plt.xlabel('Principal components')
plt.legend(loc='best')
plt.tight_layout()
plt.show()

# Make a list of (eigenvalue, eigenvector) tuples
eigen_pairs = [(np.abs(eigen_vals[i]), eigen_vecs[:, i])
               for i in range(len(eigen_vals))]
print("特徵值，特徵向量length：",len(eigen_pairs))
print("特徵值，特徵向量：",eigen_pairs)
# Sort the (eigenvalue, eigenvector) tuples from high to low
eigen_pairs.sort(key=lambda k: k[0], reverse=True)
print("特徵值，特徵向量排序：",eigen_pairs.sort(key=lambda k: k[0], reverse=True))
#保留兩個最具影響力的特徵向量組成13x2 的投影矩陣W
w = np.hstack((eigen_pairs[0][1][:, np.newaxis],
               eigen_pairs[1][1][:, np.newaxis]))
print('Matrix W:\n', w)

#畫出轉換後的數據集 散點圖
#print("X_train_std[0].dot(w)=",X_train_std[0].dot(w))
X_train_pca = X_train_std.dot(w)
colors = ['r', 'b', 'g','black','lightgreen','cyan']
markers = ['s', 'x', 'o','^','.','*']
for l, c, m in zip(np.unique(y), colors, markers):
    plt.scatter(X_train_pca[y == l, 0], 
                X_train_pca[y == l, 1], 
                c=c, label=l, marker=m)

plt.xlabel('PC 1')
plt.ylabel('PC 2')
plt.legend(loc='lower right')
plt.tight_layout()
plt.show()

##劃出決策分布圖
from matplotlib.colors import ListedColormap
def plot_decision_regions(X, y, classifier, resolution=0.02):
    # setup marker generator and color map
    markers = ('s', 'x', 'o', '^', 'v')
    colors = ('red', 'blue', 'lightgreen', 'gray', 'cyan')
    cmap = ListedColormap(colors[:len(np.unique(y))])
    # plot the decision surface
    x1_min, x1_max = X[:, 0].min() - 1, X[:, 0].max() + 1
    x2_min, x2_max = X[:, 1].min() - 1, X[:, 1].max() + 1
    xx1, xx2 = np.meshgrid(np.arange(x1_min, x1_max, resolution),
                           np.arange(x2_min, x2_max, resolution))
    Z = classifier.predict(np.array([xx1.ravel(), xx2.ravel()]).T)
    Z = Z.reshape(xx1.shape)
    plt.contourf(xx1, xx2, Z, alpha=0.4, cmap=cmap)
    plt.xlim(xx1.min(), xx1.max())
    plt.ylim(xx2.min(), xx2.max())
    # plot class samples
    for idx, cl in enumerate(np.unique(y)):
        plt.scatter(x=X[y == cl, 0], y=X[y == cl, 1],
                    alpha=0.8, c=cmap(idx),
                    marker=markers[idx], label=cl)

#使用LogisticRegression 並用兩個PCA 主成分做訓練分類 
from sklearn.linear_model import LogisticRegression
lr = LogisticRegression()
lr = lr.fit(X_train_pca, y)
plot_decision_regions(X_train_pca, y, classifier=lr)
plt.xlabel('PC 1')
plt.ylabel('PC 2')
plt.legend(loc='lower right')
plt.tight_layout()
plt.show()